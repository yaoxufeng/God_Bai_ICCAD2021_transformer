
\newpage
\section{Related Work}

\textbf{Graph-level optimization.} 
Graph-level optimizations, inspired from classical loop optimizations, is known to improve performance in other domains. They treat each operator in the computational
graph as a basic unit and implement optimization to at each subgraph without changing the intramural representation of original operators. In the high-performance computing domain, [30] formulated GPU kernel fusion as an combinatorial search problem, and search the candidate for an optimized fusion-kernel. In image processing and computer vision 
domains, [22, 23] formulated the image processing pipeline as a standard graph-cut problem. In machine learning domain, [7] introduced a kernel fusion mechanism to
generate efficient kernel code for a specific computation pattern. In the compiler domain, XLA compiler can handle more general computation pattern, but offers
only basic capability for fusion and kernel generation. However, XLA relies on empirical rules to analyze fusion possibilities, and does not support kernel generation 
with the specific operator pattern, such as elementwise, gemm, and reduction. TVM compiler can also implement operator fusion optimization. It uses Relay[] to make 
all of the rule-based graph-level optimization in compiler pass with lots of engineering workload. In the meantime, it relies on disjoint-set and
post-dominator tree in a very complex way, which may ignore lots of opportunities to improve the end-to-end performance. As for graph-level optimization, the common optimizations include layout transformation, elementwise operator fusion, constant folding, and automatic 
generation of graph substitution. The graph-level optimizations are orthogonal to the tensor-level optimizations and can be combined to boost the performance
of whole system. MetaFlow performs functional-preserving graph transformations to optimize neural network architectures. In order to reduce accesses to GPU memories,
it merges operators with the same input variable and make them implement more parallelly. TASO further designs an automated generation of substitution rules and 
it explores more mathematically equivalent neural network structures of the input one comparing to MetaFlow. MetaFlow and TASO treat the whole computation 
graph as a whole part and search for highly optimized substitution subgraphs for the original part. However, we do not want to change the structure of the original
neural network and find some new subgraphs to replace them, which seems like a neural network search on hardware. We leave the joint optimization of graph substitution 
and tensor-level optimization as future work.

\textbf{Tensor-level optimization.}
Halide proposes a scheduling language that can describe loop optimization primitives. Manual optimization and automatic search algorithm can be feasible in
this domain specific language. Currently, halide has three different versions of auto-scheduler based on various search techniques. TVM utilizes a similar scheduling
language, which is named as tensor expression, and it also requires users to design all kinds of template for different hardware backends. Optimized tensor programs
are generated by template-guided saerch framework AutoTVM. Ansor also explores kernel fusion
with tuning approach under limited patterns supported. FlexTensor introduces some general templates that can address a set of specific operators. It is obvious that
these templates can solve single operator efficiently rather than the subgraphs involving multiple operators.
People can provide schedules and Halide, TVM, Ansor, FlexTensor can tune the parameters automatically. However, 
the optimization of operator fusion is not the schedule for a single operator itself, but how to group different types of operator into fusion patterns and store 
them together into a single kernel on specific hardware backend efficiently.



\label{sec:result}
