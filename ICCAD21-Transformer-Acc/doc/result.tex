
\section{Evaluation Results}
\subsection{Experimental Setup}

We evaluate the fusion optimization and kernel generation mechanisms on three modern image recognition tasks with transformers: DETR, SETR, and ViT. %Table xxx and Table xxx summarize workload characteristics.
We use cuDNN V7.6.5, CUDA 11.0, NVIDIA driver 460.67, and adopt TensorRT V7.0.0.11 and TVM 0.8, as well as the TensorRT version as 
baselines for our performance comparison study. All evaluation results are collected on a NVIDIA GeForce RTX 2080Ti GPU. 

% We compare Automatic-E2EF against the state-of-art search frameworks and hardware-specific manual libraries. In the meantime, we evaluate the search
% efficiency of each module designed in Automatic-E2EF. 



% \subsection{Graph Fusion Analysis}

% \subsection{Kernel tunning Analysis}

\subsection{Subgraph Benchmark}
{\color{red} multi-head attention, encoder and decoder}

We perform the subgraph benchmark on two three subgraphs in tranformer. The multi-head attenion (mha) is a subgraph consisting of \textbf{h} attention heads in parallel
to attend to different learned projections of a sequence. The \textit{encoder} is a subgraph consisting of \textbf{n} identical layers. Each layer has two sub-layers. The first is a mha mechanism, and the second is a simple,
positionwise fully connected feed-forward network. The \textit{decoder} is a subgraph consisting of \textbf{n} identical layers. It inserts a third sub-layer, which performs multi-head attention over the output of 
the encoder layer. We select three different shape configurations which corresponds to the image recognition benchmarks and two batch sizes, run auto-tuning with up to xxx
measurement trails per test case, and report the normalized performance. In the meantime, we use the same set of paramters on the baseline framework. \\
Figure xxx shows that our dynamic programming operator fusion technqiue outperforms manual libraries (tensorrt) and other search frameworks (ansor )by xxx.


\subsection{End-to-End Performance}
\textbf{Workloads.} we benchmark the end-to-end inference execution time of three image recognition models with transformers, which include DETR ResNet-50
for object detection, SETR ResNet-50 for semantic segmentation, and ViT for image classification. We report the results for batch size 1. Table xxx shows the 
number of encoders, the number of decoders, and the input shape of the tensor for each network. \\



{\color{red} DETR:}

Attention mechanisms in the transformer models are the key components which model relations between feature representations of different location of objects.
%For the study 
we choose ResNet-50-based DETR model with \textbf{6} encoder, \textbf{6} decoder layers and width \textbf{256}. 
To be comparable in the number of paramters we choose a model with 6 encoder and 6 decoder layers of width 256 with 8 attention heads. \\

{\color{red} SETR:}
asada \\

{\color{red} ViT:}
ASDASD \\



\textbf{Baselines.} asdada \\
\textbf{Results.} asdasd \\
\textbf{Abliation study.} asdasd \\



\label{sec:result}


\newpage
\begin{table*}[htbp]
    \caption{architecture of the model}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{l|l|l|l|l|l|l|l|l|l|l}
    \hline
    model               & encoder & decoder & width & nhead & input shape           & patch & transformer input                                                                     & mha input                                                                                                            & encoder input          & decoder input                                                                             \\ 
    \hline
    \hline
    resnet50-based DETR & 6       & 6       & 256   & 8     & {[}1, 3, 800, 1333{]} & N/A   & \begin{tabular}[c]{@{}l@{}}src {[}1050,1, 256{]} \\ tgt {[}100,1, 256{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}query {[}1050, 1, 256{]}\\ key {[}1050, 1, 256{]}\\ value {[}1050, 1, 256{]}\end{tabular} & src {[}1050, 1, 256{]} & \begin{tabular}[c]{@{}l@{}}tgt {[}100, 1, 256{]}\\ memory {[}1050, 1, 256{]}\end{tabular} \\
    \hline
    SETR-Naive          & 24      & 1       & 1024  & 16    & {[}1, 3. 768, 768{]}  & 16    & \begin{tabular}[c]{@{}l@{}}src {[}2304,1, 1024{]} \\ tgt {[}2304, 1, 1024{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}query {[}2304, 1, 1024{]}\\ key {[}2304, 1, 1024{]} \\ value {[}2304, 1, 1024{]}\end{tabular} &   src {[}2304, 1, 1024{]}    &   {[}2304, 1, 1024{]}                                 \\
    \hline
    ViT-Base-16         & 12      & 0       & 768   & 12    & {[}1, 3, 224, 224{]}  & 16    & \begin{tabular}[c]{@{}l@{}}src {[}197, 1, 768{]} \\ tgt {[}197, 1, 768{]}\end{tabular}    & \begin{tabular}[c]{@{}l@{}}query {[}197, 1, 768{]}\\ key {[}197, 1, 768{]} \\ value {[}197, 1, 768{]}\end{tabular}  &     src {[}197, 1, 768{]}   &     N/A                                                    \\ 
    \hline
    \end{tabular}
    }
\end{table*} 


\begin{table*}[htbp]
    \caption{batch matrix multiplication and softmax in MHA}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
    layers (encoder) & GFLOPS & \#params & mAP  & PyTorch JIT & TensorRT & Ansor & Our method  \\ \hline
    3                & 81     & 37.4M    & 40.1 & 5.64        & 2.87     & 2.71  & 2.37        \\ \hline
    6                & 86     & 41.3M    & 40.6 & 11.45       & 5.65    & 6.13  & 5.25         \\ \hline
    12               & 95     & 49.2M    & 41.6 & 23.17       & 11.37    &  12.35     &  9.98            \\ \hline
    \end{tabular}
\end{table*}


\begin{table*}[htbp]
    \caption{dynamic programming for operator fusion on DETR}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
             & encoder & weights & decoder & weights & transformer & weights  & measurement trails \\ \hline
    Ansor    &     9                &   [6, 6, 6, 6, 6, 12, 12, 6, 6]  &       13   & [18, 6, 6, 6, 6, 18, 18, 6, 6, 6, 6, 6, 6]  &           22               &      [18, 6, 6, 6, 6, 19, 19, 6, 6, 6, 6, 6, 6, 6, 6, 6, 13, 13, 6, 6, 6, 6]                  &        10000            \\ \hline
    AutoGTCO &              &                        &                      &                     &                          &                        &       8790             \\ \hline
    \end{tabular}
    }
\end{table*}


\begin{table*}[htbp]
    \caption{DETR Acceleration}
    \centering
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
                & MHA  & Encoder & Decoder & Transformer & Speedup \\ \hline
    PyTorch JIT & 0.53 & 12.96   & 15.75   & 23.67       & -   \\ \hline
    TensorRT    & 2.05 & 5.65    & 5.65    & 7.73        & 1.00    \\ \hline
    Ansor       & 1.07 & 6.13    & 1.58    & 6.78        & 1.12x   \\ \hline
    AutoGTCO    & 0.83 & 5.25    & 1.35    & 5.60        & 1.27x   \\ \hline
    \end{tabular}
\end{table*}


\begin{table}[htbp]
    \caption{image recognition with transformers acceleration}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{|l|l|l|l|l|}
    \hline
                   & PyTorch JIT & TensorRT & Ansor & AutoGTCO \\ \hline
    DETR-ResNet-50 & 23.67       &  7.73    & 6.78  & 5.60     \\ \hline
    SETR-Naive     &             &          &       &          \\ \hline
    ViT-Base-16x16 &             &          &       &          \\ \hline
    \end{tabular}
    }
\end{table}